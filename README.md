ECSTaskAnalyzer Lambda Function Overview This AWS Lambda function analyzes recently stopped ECS tasks for a specified ECS service. fetches ECS stopped tasks information, and identifies if tasks stopped due to maintenance or scaling. If a stopped task's reason is unexpected, it can trigger an alert (e.g., PagerDuty). 1. Imports and Package Setup We import packages needed for JSON handling, HTTP requests, AWS Lambda, AWS SDK for ECS and Secrets Manager, context handling, etc. The package is main as this is a standalone executable (AWS Lambda). 2. Global Variable Environment is a string variable that will hold the environment name (like prod, devcd) 3. Data Structures - TaskResult: Struct to hold info about each ECS task stopped - SplunkBody: Struct representing the inner event body sent by Splunk with fields like Severity, Detector, ServiceName, etc. 4. Helper Functions - isMaintenance(reason string) bool: Returns true if the stop reason contains the word indicating maintenance - isScaling(reason string) bool: Returns true if the stop reason contains the word indicating scaling 5. sendPagerDutyAlert Function This sends an alert to PagerDuty if a task stopped for an unexpected reason. It creates a JSON payload containing details like severity, detector, task ARN, etc. Sends HTTP POST request to PagerDuty’s events API. Checks the response — expects HTTP 202 Accepted, else returns an error. 6. getSecret Function Uses AWS Secrets Manager API to fetch a secret. It retrieves a secret by name, parses the JSON secret string, and extracts the PagerDuty routing key. Returns the key or an error if something fails. 7. handler Function — The Lambda Handler - Input: Takes a JSON event, AWS ECS client, and PagerDuty secret key. Parses the outer event JSON to get the inner event Body string. Parses the inner Body string (expected to be a Splunk event) into SplunkBody struct. Prints details from the Splunk event for logging. Validates that the ServiceName field is not empty; if empty, returns error. Creates a timeout context of 10 seconds to enforce Lambda handler time limits. Calls ECS ListTasks API (ListTasks) to get stopped tasks for the given service in the specified ECS cluster (Environment). If no stopped tasks found, logs this and returns an empty list immediately. Calls ECS DescribeTasks API (DescribeTasks) to get detailed info about the stopped tasks. - For each stopped task: Checks if context is cancelled (timeout). Extracts task ARN and stop reason. Checks if stop reason is due to maintenance or scaling. Appends task info to results list. If stop reason is NOT maintenance or scaling, triggers PagerDuty alert by calling sendPagerDutyAlert() Returns the list of TaskResult structs to AWS Lambda runtime. 8. main Function Loads AWS SDK default config. Creates clients for AWS Secrets Manager and ECS. Reads the environment name from environment variables. Fetches PagerDuty secret key from Secrets Manager using getSecret. Starts the Lambda function using lambda.Start() and wraps the handler function to pass ECS client and secret value. Summary: What this Lambda does Triggered by an event (likely from Splunk or CloudWatch). Parses the event to extract ECS service name and details. Lists stopped ECS tasks for that service. Describes stopped tasks for detailed reasons. Checks if task stopped due to maintenance/scaling or unexpected reasons. If unexpected, sends alert to PagerDuty. Returns info about stopped tasks.

terraform : 
-----------
Designed and implemented standalone AWS Lambda Golang function (ECSTaskAnalyzer) to monitor ECS services by analyzing recently stopped ECS tasks and identifying root causes (scaling, maintenance, or unexpected failures). Used ECS APIs (ListTasks, DescribeTasks) and Secrets Manager (GetSecretValue), and triggered alerts through PagerDuty Events API (POST). Integrated with PagerDuty to trigger automated alerts for unexpected task failures, improving incident response time. Triggered the Lambda through Splunk webhook for real-time monitoring. Maintained a Makefile to automate Go binary builds, code quality checks (vet, test, install, build), packaging, and deployment to AWS S3. Deployed Go-based Lambda functions by fetching build artifacts from S3 and provisioning infrastructure through Terraform (init, plan, apply) with S3 remote backend. we provisioned the Lambda using Terraform, configured image overrides, and fetched the build tag for the image from S3 Tech Stack: AWS Lambda, ECS, S3, Secrets Manager, CloudWatch, Splunk, PagerDuty, Go (Golang), HTTP APIs, Terraform. this is my first task and second task I worked on terraform modularised deployments this is the second task doc Modularised Deployment By Sudheer Debbati 5 min 4 Add a reaction Overview What is Modularisation? Modularisation in Terraform means splitting the infrastructure code into smaller, reusable, and manageable pieces called modules. Goal of Modularisation: to refactor the Terraform codebase into separate modules, one per service, to improve reusability, maintainability, and scalability. Before Modularisation: All components were defined in a single Terraform template folder. Deploying even a small change in one component required applying the entire stack. Components were tightly coupled and environment-specific. Before Folder Structure: deployments/ └── devcd/ └── input.json templates/ ├── Makefile ├── alb.tf ├── autoscale.tf ├── createIPSets.py ├── datadog.tf ├── ecAlarms.tf ├── eventCollector.tf ├── eventCollectorContainer.tpl ├── iam.tf ├── overrides.tf.json ├── remote.tf ├── splunkDetectors.tf ├── terraform.tfvars.j2 ├── variables.tf.j2 ├── versions.tf.j2 └── waf.tf Issues in This Setup: All services (ALB, WAF, Datadog, Splunk, etc.) were bundled in one place. A small change in one service required re-applying the entire stack. No modular reuse between environments (devcd/prod/ etc.). Difficult to test, debug, or isolate issues. After Modularisation: Each component (ALB, WAF, ecservices, splunk, datadog, infra) has its own independent module folder. Now we can deploy only the component that changed without touching others. We can deploy the same component (e.g., ALB) with a different setup for different environments. After Modularisation – Modular Structure Now, each service (ALB, Datadog, ecservices, Infra, WAF, Splunk) is separated into its own subfolder under templates/. Each folder contains its own .tf files. New Structure: deployments/ ├── ec/ │ ├── devcd/ │ │ ├── input.json │ │ ├── ise/ │ ├── devcd/ │ │ ├── input.json │ │ templates/ ├── alb/ │ ├── alb.tf │ ├── variables.tf │ ├── terraform.tfvars.j2 │ ├── versions.tf.j2 │ └── outputs.tf ├── datadog/ ├── ecservices/ ├── infra/ ├── splunk/ └── waf/ Each module can now be independently deployed or reused in any environment with different configurations. State File Management Before and After Modularisation Terraform state file is like a snapshot that keeps track of what infrastructure Terraform created, so it knows what to change, update, or destroy during the next run. Before Modularisation: All resources shared a single Terraform state file, which made it risky — a minor issue in one resource could affect the entire infrastructure state. s3://visibility-dev-state/devcd-vis/cd-eventing/eventCollector/ └── terraform.tfstate After Modularisation: Each component/module has its own state file (via backend configuration), enabling: Independent state management Easier collaboration (reduced locking conflicts) Safer deployments with isolated impact s3://visibility-dev-state/dev-ise/ise-testing/ise/ ├── alb/terraform.tfstate ├── waf/terraform.tfstate ├── datadog/terraform.tfstate ├── ecservice/terraform.tfstate Inter-Module Dependency Using Outputs.tf and remote.tf If one module depends on another (e.g., ecservice needs the ALB DNS name), we don’t hardcode values. Instead: The providing module (e.g., alb) exposes necessary values using output blocks. The dependent module fetches those values using terraform_remote_state data blocks. Example: In templates/alb/outputs.tf (Exposing the value): output "target_group_arn" { value = aws_alb_target_group.ise-alb_target_group.arn } In templates/ecservice/remote.tf (Consuming the value): data "terraform_remote_state" "alb" { backend = "s3" config = { region = var.region bucket = "${var.account}-state" key = "${var.environment}/${var.service}/ise/alb/terraform.state" encrypt = true } } # Use the output in ECS or other resources target_group_arn = data.terraform_remote_state.alb.outputs.target_group_arn Deployment Flexibility with Modularisation Selective Deployment: Only deploy the component that changed no need to worry about other old deployments Reusable Components: The same ALB module can be reused in devcd, test, and prod environments by just passing different variables. Easy Maintenance & Future Growth Easier testing and rollback. Changes are localized to one module. Scalable for New Services: Going forward, if we add more services: Just create a new folder in templates/ Add the related .tf files No need to modify or worry about existing components. Template Rendering: Render Jinja2 Templates using createDeployment.py Rendering automates environment-specific configuration by injecting values from input JSON into reusable templates, enabling consistent, multi-environment, and scalable deployments. Full Stack: python3 createDeployment.py -t templates -i devcd/input.json -d devcd --fullStack This will render all components: infra, alb, datadog, splunk, ecservice, waf Single Component: python3 createDeployment.py -t templates -i devcd/input.json -d devcd -a alb Only the selected component will be rendered in devcd/alb/ Terraform Deployment: After rendering, navigate to the target component folder and run Terraform: cd devcd/<component> # Example: cd devcd/alb terraform init # Initialize backend and providers make plan # See planned changes make deploy # Apply the changes Repeat the same steps for any other component folder (e.g., infra, ecservice, datadog, etc.) as needed. Overall, this modular approach leads to cleaner architecture, faster deployments, safer operations, and easier scalability as we grow. Future services can be added easily by creating a new folder with its own Terraform files. Summary: Benefit Description Selective Deployment Deploy only changed modules (e.g., ALB only) Multi-Env Reusability Use same module in different environments Scalable Structure Add future services without touching old ones Input-Based Execution Controlled and safe deployments using input files Easier Maintenance Reduced risk and faster debugging Jenkins Pipeline Implementation and Job Integration This explains how the Jenkins pipeline is set up, what parameters it uses, what scripts it calls, and how to use it to deploy components. 1️. Overview This Jenkins pipeline automates deployment by: Checking out a specific Git commit (or a default branch). Preparing the environment and directories. Deploying selected components to a chosen stack (devcd, integ3 or etc). Updating Terraform overides.tf.json files automatically with the right artifact tags. 2️. Pipeline Stages Here are the main stages of the pipeline: Environment Prepare Workspace folders, Terraform plugin cache, AWS credentials, Environment variables needed. for deployment. Checkout Gets the code, Uses the specific Git commit (GIT_HASH) if we provided one. Otherwise, defaults to */DeployJenkinsfile branch(latest commit). Deploy Decides Which components we selected to deploy. Which artifact tags apply to component. Runs: createDeployment.py to generate Terraform files. updateStack.sh to update overides.tf.json file and run make plan/deploy. 3️. Jenkins Job Details Item Value Job Name TestJenkinsFile_Modulerization Repository git@github.com:cisco-sbg/cspe-event-collector.git Branch DeployJenkinsfile Jenkinsfile DeployJenkinsfile Note: This Jenkinsfile was previously named ‘MergeDevCDJenkinsfile'. It was renamed to 'DeployJenkinsfile' because the pipeline is now generic and supports multiple stacks dynamically, not limited to 'devcd'. Use this file for deployment across different stacks/environments. 4️. Pipeline Parameters This pipeline uses several parameters to customize the build: GIT_HASH: Use a specific Git commit hash. If blank, the pipeline uses the DeployJenkinsfile branch (latest commit). STACK: Select where to deploy (devcd, integ3 or etc). DEPLOYMENT_SET: EC ISE Input Files in the Deployments Folder Inside the deployments folder, deployment configurations are organized by deployment set and stack, for example: deployments/ ├── ec/ │ ├── devcd/ │ │ ├── input.json │ │ ├── ise/ │ ├── devcd/ │ │ ├── input.json │ │ How the pipeline uses these files: If you select ISE in the DEPLOYMENT_SET parameter, the pipeline uses: deployments/ise/<STACK>/input.json If you select EC in the DEPLOYMENT_SET parameter, the pipeline uses: deployments/ec/<STACK>/input.json Purpose: These files contain different deployment configurations specific to each deployment set and environment stack. scalability: In the future, if you want to add another deployment set, you can simply create a new folder under deployments/ (e.g., deployments/mynewset/) with the same structure (<deployment set>/<stack>/input.json). The pipeline will dynamically pick up the correct input file based on the DEPLOYMENT_SET and STACK parameters. COMPONENTS: Select which components to deploy Behavior: The available options change automatically based on DEPLOYMENT_SET Available Components: Deployment Set Components EC infra, alb, waf, ecservice, splunk ISE infra, alb, waf, ecservice, splunk, datadog Notes: we can select one or multiple components. The list updates live when we change the deployment set. ARTIFACT_TAGS: Set Docker image tags for components. Format: component=tag Example: ecservice=release123 Default: ecservice=CD_PASSED 5️. Scripts These scripts run during deployment: scripts/updateStack.sh Purpose: Goes through each selected component Figures out which image tag to use Updates the Terraform overrides.tf.json file Runs make plan/deploy Key Details: Accepts components as params If no components are passed, defaults to deploying all For each component: Checks if an artifact tag environment variable is set (*_ARTIFACT_TAG) If not, defaults to CD_PASSED Updates overrides.tf.json Runs make plan/deploy (with retry on failure) scripts/updateOverrides.py Purpose: Updates overrides.tf.json to set the Docker image tag for a component (ecservice) Reads the file, updates the tag, saves it back 6️. Run the Jenkins Job Step by step: 1️. Set Parameters: GIT_HASH: leave empty for default branch(latest commit) or enter a specific commit hash STACK: choose devcd or integ3 or etc DEPLOYMENT_SET: select EC or ISE COMPONENTS: select one or more components (list updates automatically based on DEPLOYMENT_SET) ARTIFACT_TAGS: define tags if you want custom images 2️. Click “Build Now.”
